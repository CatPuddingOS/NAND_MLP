A CONTINUATION OF NAND MODEL TRAINING
    - This project is for the purpose of studying multi layer perceptron models
    - The work is a continuation of the previous work done on the NAND single layer perceptron

10/21:
    - Dropped in the perceptron class from quick_perceptron (original project)
        - Added polish and separated some functionalities, works well
    - Incorperated the beginnings of the model class
        - Will host perceptrons and layers
        - Encountered issues with populating. Iterating on and defining where perceptrons
          are placed will be difficult with the setup
    - Current guesses on structure:
        - Another class , layer -> owns perceptrons
        - modele -> will own layers
        - creates a nice hierarchy

THE QUESTION:
    Can what was failed by the single perceptron be achieved by a 2 layer perceptron by using the same dataset. SPOILER: Not in any way I've thought of
    * I'm looking for understanding of a decimal that's been included into the set, 0.5.
      The single perceptron bacame incredibly confused and experienced platueas in guess rates.
      My hope is that this will be avoided with this new model.
    HYPOTHESIS:
    * I have nothing to compare this to and have few expectations on the results. However, reading and my 
      generic understanding of linear data leads me to believe that it may be possible for two perceptrons
      to form a higher understanding of the data. 
      I am assuming the model will converge on a way to split the work between both perceptrons (now called nodes)
      which will make the problem linear between the two of them.

10/22:
  * Work is hellishly slow, realizing now that I do not understand npz files all that well.
  Some things were added / improved / changed
  1. describe() now walks up the class hierarchy:
    - violently hard to follow and could probably be reduced to a single call with the correct syntax
      which I may look into.
  2. The NAND_perceptron class was changed a bit to accomidate the dumpster fire called model.load()
  3. model.save() and model.load()
     - save works great. Same logic that the single layer quick_preceptron used to save the npz in my last
       project.
     - load is abysmal and poorly thought out. I got tired of the reading for npz files and landed
       on an idea that worked but required some finagling and bad practice. 
     - Final result is a system that is capable of saving and loading a model of pretty much any size
       from an npz.

  Some things to consider/research going forward:
  1. Layers of differing sizes
     - Some layers will gradually grow in size ie. input -> 3 node layer -> 4 node layer -> 3 node layer -> output
       I have no idea what this means or why the size changes. Guess: Innermost hidden layers are most likely 
       to learn new behavior? So more are present to potentially capture the behavior? As a perceptron model, 
       it is likely irrelevant. This concept is likely better suited for sigmoid models, though I should do
       more research. For now, having multiple nodes per layer is good for practice on my part.

10/23:
  Whoops
    * My datasets were never sending in the first row of information
      1,1,1,1 = 0 was not being evaluated becasue I neglected to include column identifiers
      Oopsie

  On the current content
    * Fit seems to be hitting all perceptrson correctly and talking to related functions as expected.
      Next up are forward functions to link up nodes and send outputs for actual guessing to take place.
        - Update: that didn't happen. Forward is just a part of fit now...

10/24:
  Fundamental misunderstandings
    * I have an obvious misunderstanding of ho layers operate in the grand scheme of a model or at least
      what the terminology signifies. A "two layer perceptron" is one such that the model contains:
      1. The input layer
        - No computation, just data from a user or dataset
      2. Hidden layer
        - Layer of one or more nodes that can be activated to return a dot product of weights and inputs
      3. Output layer
        - Group of one or more nodes that can be activated to return the final guess of the model
    
    * I need to rewrite a lot of what I have

    * More plateaus in return values for fit. Everything seems to be in order. My guess is that 0.5 contributes
      too much to the dot. More layers might be the way to go? I'm a little lost at this point.

    Idea: 
      - Create something that looks for plateaus in guess rates. Once a plateau is "found," add another layer
        of nodes to the model. Another layer may or may not figure this out.
        My guess is that the model will eventually converge or explode into a massive array of nodes. RAM useage
        will tell once I'm done.
      
    * Run 100 trials with a sub 100% rate ? layers += debugGenerateLayer(x)
      - Add 1 layer with x perceptrons @ 4 inputs each 
    * Absolutely did not work. Same issue with even stranger results with 100+ layers of 4 perceptrons.

10/25:
  Project is mostly dead at this point, any further work will be for entertainment
  * Some fun things i've figured out. The finctionality of creating other gates from NAND is retained
    AND is a flip of NAND, so passing the output of 4 NAND perceptrons to a single perceptron as input 
    yields AND functionality. Likewise for other tested structures

  * Beyond this, I'm chalking this up to a flop. Will clean up and play w/ Simoid activation

