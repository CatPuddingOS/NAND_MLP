A CONTINUATION OF NAND MODEL TRAINING
    - This project is for the purpose of studying multi layer perceptron models
    - The work is a continuation of the previous work done on the NAND single layer perceptron

10/21:
    - Dropped in the perceptron class from quick_perceptron (original project)
        - Added polish and separated some functionalities, works well
    - Incorperated the beginnings of the model class
        - Will host perceptrons and layers
        - Encountered issues with populating. Iterating on and defining where perceptrons
          are placed will be difficult with the setup
    - Current guesses on structure:
        - Another class , layer -> owns perceptrons
        - modele -> will own layers
        - creates a nice hierarchy

THE QUESTION:
    Can what was failed by the single perceptron be achieved by a 2 layer perceptron by using the same dataset.
    * I'm looking for understanding of a decimal that's been included into the set, 0.5.
      The single perceptron bacame incredibly confused and experienced platueas in guess rates.
      My hope is that this will be avoided with this new model.
    HYPOTHESIS:
    * I have nothing to compare this to and have few expectations on the results. However, reading and my 
      generic understanding of linear data leads me to believe that it may be possible for two perceptrons
      to form a higher understanding of the data. 
      I am assuming the model will converge on a way to split the work between both perceptrons (now called nodes)
      which will make the problem linear between the two of them.

10/22:
  * Work is hellishly slow, realizing now that I do not understand npz files all that well.
  Some things were added / improved / changed
  1. describe() now walks up the class hierarchy:
    - violently hard to follow and could probably be reduced to a single call with the correct syntax
      which I may look into.
  2. The NAND_perceptron class was changed a bit to accomidate the dumpster fire called model.load()
  3. model.save() and model.load()
     - save works great. Same logic that the single layer quick_preceptron used to save the npz in my last
       project.
     - load is abysmal and poorly thought out. I got tired of the reading for npz files and landed
       on an idea that worked but required some finagling and bad practice. 
     - Final result is a system that is capable of saving and loading a model of pretty much any size
       from an npz.

  Some things to consider/research going forward:
  1. Layers of differing sizes
     - Some layers will gradually grow in size ie. input -> 3 node layer -> 4 node layer -> 3 node layer -> output
       I have no idea what this means or why the size changes. Guess: Innermost hidden layers are most likely 
       to learn new behavior? So more are present to potentially capture the behavior? As a perceptron model, 
       it is likely irrelevant. This concept is likely better suited for sigmoid models, though I should do
       more research. For now, having multiple nodes per layer is good for practice on my part.
     - 
